---
title: Image Generation
category: Methodology
order: 4
---

The final stage of our pipeline is the Image Generation module. It takes as input the layouts generated by the [layout-generation module](../Layout-Generation) and generates a corresponding image. As mentioned in the [related works](../../Related Work), we use [AttrLostGan](https://arxiv.org/pdf/2103.13722.pdf) for generating the images. The architecture and workings of the method are described below.

<center>
<img src="../../images/attrlostgan.png" alt="example" style="width:1000px;"/>
<br>
</center>

Given an image layout consisting of a set of objects along with their bounding boxes and object attributes, the goal of the model is to generate a natural-looking image with accurately positioned objects according to their bounding boxes, also reflecting their input attributes. The proposed approach first samples latent vectors for the entire image and the individual objects to control the style of the generated image and objects. The latent vectors are sampled from the standard normal distribution. These latent vectors are passed through a ResNet-based generator to generate the final image. The generator is also conditioned by the Attr-ISLA module which generates two transformation parameters gamma and beta that incorporate information about the object positions from the bounding boxes and the attributes.

The discriminator consists of three components: a shared ResNet backbone as a feature extractor, an image head classifier, and an object head classifier. The object-label and object-attribute embeddings are passed to the object head to predict the object-label and object-attribute scores. The final loss is calculated as a weighted sum of the adversarial hinge losses on the image, object-label, and the object-attribute. The model is trained on the [Visual Genome dataset](../../Datasets). We use the pre-trained model weights provided in the [official repository](https://github.com/stanifrolov/AttrLostGAN) to generate the images for our experiments. However, during inference, attributes for the newly added objects to the scene graph might not be available. For those cases, we sample the object-attributes from the Visual Genome dataset.



